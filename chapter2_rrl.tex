\chapter{Review of Related Literature} \label{sec:rrl-title}
\section{Great Deluge-Based Works}
Many previous works dealing with course timetabling utilize an optimizing heuristic (or derivatives of it) called \textbf{Great Deluge}. Great Deluge was introduced by Gunter Dueck, and is a heuristic that is similar to Hill Climbing and Simulated Annealing. To understand how it works, imagine that you are in a point in some area with mountainous terrain. This area constitutes your solution space, with higher points in the area having higher values and, thus, having a better solution. The initial point you are located in in the area represents the initial solution that is generated. Imagine as well that it is raining endlessly and the water level $W$ is continuously rising at a constant rate $R_{w}$, where $R_{w}$. $W$ can start at any value that is greater than $0$. Assuming that we are attempting to maximize some function $Q$, which evaluates a solution based on some criteria, your goal is to locate the relatively highest point in the location. This point can be thought of as the local optimum. Locating the highest point involves walking around the area that is not below the current water level. This will force you to walk to a higher and higher point since $W$ is constantly rising. Once you are no longer able to proceed to a higher level, it means that you are now in a local optimum. Going outside the analogy and back to a technical perspective, this local optimum would now be the \textit{relatively} best solution for your problem. Every "walk" or move to a higher point is nuanced relative to the analogy. A single walk means construction of a new solution $S_{new}$ with basis on the current solution $S_{current}$. If $Q(S_{new}) \geq W$ \cite{gd-burke}, then we accept $S_{new}$ as the new current solution and "walk" towards it, and we increase $W$ by $R_{w}$. Otherwise, we simply generate a new $S_{new}$. These walks are performed until $Q(S_{new})$ is not greater than $Q(S_{curr})$ for a long time or we have reached the maximum number of moves/iterations \cite{intro-gd-dueck}. Great Deluge can also be adapted to minimize $Q$. Instead of $W$ increasing, it will be decreasing by the same rate. A solution $S$ will now be accepted if $Q(S) \leq W$. Conversely, the algorithm will stop when $Q(S_{new})$ is not lesser than $Q(S_{curr})$. The second stopping condition for the algorithm still applies in this case \cite{gd-burke}\cite{nlgd-landa-silva}\cite{nlgdrl-obit}. This minimization case is the one adapted by Great Deluge-based works that focus on course timetabling.

% Insert algorithm for GD.

One of the earliest works that uses Great Deluge was that of Burke, E., Bykov, Y., Newall, J., and Petrovic, S. \cite{gd-burke}. Their work explores the use of Great Deluge in university course timetabling. In their work, they defined $Q$ as the sum of the number of soft constraints violated. They also, however, slightly modified Great Deluge. We will be referring to the modified version as the Extended Great Deluge (EGD). Instead of simply obtaining a single new solution $S$ in an iteration and comparing $Q(S)$ to $W$. Burke, E., et. al. extended Great Deluge to take multiple neighbouring solutions $N$ on every iteration. A random solution $S_{N}$ from $N$ will be taken and it will be the one to be compared to $W$ and the current solution. $S_{N}$ will be accepted as the current solution if $Q(S_{N}) \leq Q(S_{curr})$. $S_{N}$ can still be accepted only under the condition that $Q(S_{N}) \leq W$. In addition to taking multiple solutions in an iteration, a few more extensions have been added to Great Deluge by Burke, et. al.. The initial value for $W$ is set to $Q(S_{i})$, where $S_{i}$ is the initial solution. Computing for $R_{w}$ is made easier by using the following equation:

\[
    R_{w} = \frac{W - Q(S^{'})}{N_{moves}}
\]

$N_{moves}$ denotes the number of moves the algorithm will perform before terminating. The desired number of violations the desired solution $S^{'}$ should have is denoted by $Q(S^{'})$. Through their work, Burke, E., et. al. observed that there is a tradeoff between the quality of produced timetables and the amount of searching time given to the algorithm. The authors noted that even though in some situations, a user would require obtaining the results quickly, it is naturally more preferable that one gives the algorithm more in order for it provide high quality timetables. As the authors have mentioned, course timetabling normally only occurs once or twice in a year. Thus, a long amount of time for searching "seems to be quite acceptable". EGD was compared to Simulated Annealing (SA), Threshold Acceptance (TA), Hill-Climbing (HC), and 21 algorithms submitted for the International Timetabling Competition (ITC) of 2002. The data set used for comparing the algorithms was the one provided by the aforementioned competition The experiments of Burke, E., et. al. show that EGD produce less scattered results than SA, TA, and HC. This proves the effectiveness of EGD as a heuristic for course timetabling. Further proof of this effectiveness is shown by comparing the heuristic to the 21 other algorithms participating in the ITC. EGD generated the best results for 8 out of 23 data sets among the participating algorithms \cite{gd-burke}.

Building upon the work of Burke, et. al. is that of Landa-Silva, D., and Obit, J.'s \cite{nlgd-landa-silva}. The primary contribution of Landa-Silva, D., and Obit, J. is the introduction of a modification of the Extended Great Deluge by Burke, E., et. al., the Non-Linear Great Deluge. Key to this heuristic is the use of an equation, instead of a constant rate, in determining the amount of reduction in the water level in an iteration. We refer to this amount as the decay rate. The next water level $W$ is computed with the following equation:

\[
	W = W \times (exp^{-\delta(rand[min, max])}) + P	
\]

$P$ is "the minimum expected penalty corresponding to the best solution". Another key part of the equation is $exp^{-\delta(rand[min, max])}$, which controls the speed the water level decreases. Besides from contributing an equation-based decay rate for the water level, another modification they added to the Extended Great Deluge is the conditional increase of the water level. If the current solution obtained $S_{curr}$ is about to converge with the water level $W$, i.e. when $W - Q(S_{curr}) < 1$, then the Non-Linear Great Deluge algorithm allows the water level to rise for a certain amount. This amount is a random value from the interval $[W_{min}, W_{max}]$. The intended increase is to allow the algorithm to "accept slightly worse solutions to explore different areas of the search space in the hopes of finding better solutions". The work of Landa-Silva, D. and Obit, J. uses three moves in generating solutions:

\begin{itemize}
	\item \textbf{M1}: Selects one random class and gives it a random but feasible timeslot-room pair.
	\item \textbf{M2}: Selects two random classes and swaps their timeslot-room pairs while maintaining feasibility.
	\item \textbf{M3}: Looks for a class which violates soft constraints based on their current timeslot-room pair. It then moves this class to a random timeslot-room pair but still maintaining feasibility.
\end{itemize}

It should be noted that these three neighbourhood moves always maintain compliance with the hard constraints. Another key modification the authors introduced is the three-step heuristic for generating an initial solution. This heuristic is based on the work by Chiarandini, M., Birattari, M., Socha, K., and Rossi-Doria, O. \cite{chiarandini-effective-hybrid}. The three steps are as follows:

\begin{enumerate}
	\item All classes will first be assigned a random timeslot. However, unassigned classes with the highest number of conflicts will be assigned first. A class has a conflict with another class if it has students also taking the other class. The maximum bipartite matching algorithm \cite{maximum-bipartite-matching} is then used to assign each event to a room. This first step of creating an initial timetable $S$ does not guarantee feasibility. But the solution will be further improved by the later steps.
	\item Moves M1 and M2 are then used to improve the current $S$. At this step, feasibility and satisfaction of the hard constraints are sought. As such, "a move is only accepted if it improves the satisfaction of the hard constraints". This step performed continuously until there are no more improvements to $S$ after 10 iterations.
	\item Tabu search \cite{brownlee-tabu-search} is then used to further refine $S$. In this step, classes that were assigned $t_{iter}$ iterations ago will be stored in the tabu list. $t_{iter}$ is computed as $t_{iter} = ran(10) + \delta \times N_{v}$, where $ran(10)$ is a random number from the interval $(0, 10)$, $\delta = 0.6$, and $N_{v}$ is the number of classes that partook in violating the hard constraints. The termination condition for this step is when after 500 iterations, no solution has been produced that is better than the current best.
\end{enumerate}

Only step 1 is run once. The other steps, steps 2 and 3, are performed repeatedly in order until a feasible solution/timetable is obtained. In all 11 experiment instances obtained from the work of Socha, K., Knowles, J., and Sampels, M. \cite{socha-maxmin-ant-system}, which has 5 small instances, 5 medium instances, and 1 large instance, Non-Linear Great Deluge produced results better than Great Deluge. Interestingly enough, in 4 of these experiment cases, NLGD performed the best compared to the best known literature at the time NLGD was introduced \cite{nlgd-landa-silva}.

\section{Machine Learning-Based Works}
All related works encountered that use machine learning that were proposed do not use a purely machine-based approach. Rather, they combine heuristics with machine learning.

An example of such works, is that of Obit, J., Landa-Silva, D., Sevaux, M., and Ouelhadj, D. \cite{nlgdrl-obit}. Their work is built upon their previous work on Non-Linear Great Deluge for course timetabling \cite{nlgd-landa-silva}. An addition to their work compared to previous studies is the use of reinforcement learning as part of their solution generation process. Previous works, relative to their's, randomly select moves. But, Obit, J., et. al. uses reinforcement learning to determine which moves is best to further refine the current solution. Two types of reinforcement learning are employed and investigated in their work: \textbf{(a)} reinforcement learning with static memory length, and \textbf{(b)} reinforcement learning with dynamic memory length. Initially, all moves are given equal weights of $0.01$, with the weight for each move $i$ denoted by $w_{i}$ and thus, equal probabilities of being chosen for an iteration. The probability for each move is denoted by $p_{i}$ is computed by

\[
	p_{i} = \frac{w_{i}}{\sum_{i=1}^{n}w_{i}}
\]

$n$ is the number of moves there are. In the type with static memory length, on every iteration, the moves are punished or rewarded depending on their performance. A move is rewarded by giving its weight 1 point. It is punished by giving it no points at all. The weights are updated every learning period $lp$ computed using $lp = max(N_{moves}/500, n)$, where the total number of feasible moves performed is denoted by $N_{moves}$. During this learning period, the water level is also increased by a certain amount. Performance of each move is computed based on the number of it was called, number of times it generates solutions that has different fitness values, and the number of times it produces solutions that have been accepted. On the other hand, reinforcement learning (RL) with dynamic memory length, the probability $p_{i}$ of a move being chosen is computed through

\[
	p_{i} = \frac{w_{i} + w_{min}}{\sum_{i=1}^{n}{w_{i} + w_{min}}}
\]

In this equation, $w_{min} = min(0, w_{i})$. Unlike its counterpart, this type of RL updates the weights of the moves every time a feasible move is performed. The performed move is rewarded when it produces an improve solution, and punished otherwise. Differing from its counterpart once again, this type of RL uses a piecewise function $R$ to compute the reward/punishment for the currently selected move $i$ at the current iteration $j$. In the function, $\Delta$ is the difference between the best solution so far and the current generated solution.

\[
	R = 
	\begin{cases}
	1 & \text{if}\,\Delta < 0 \\
	-1 & \text{if}\,\Delta > 0 \\
	0.1 & \text{if}\,\Delta = 0\,\text{and new solution} \\
	-0.1 & \text{if}\,\Delta = 0\,\text{and no new solution} \\
	\end{cases}
\]

Each weight $w_{i}$ is then computed at each iteration $h$ with the formula below

\[
	w_{i} = \sum_{j = n_{timeslots}}^{h} \sigma^{j}R
\]

$n_{timeslots}$ is the number of timeslots there are. The parameter $\sigma$ is a random value between $(0.5, 1.0]$. The parameter's value is set every learning period $lp$. Similar to the previous type of RL, the $lp$ is still determined using the same formula, and the water level increases on every learning period. The experiments of this work included an experiment comparing the two types of reinforcement learning employed. Both types of RL produced optimal results in the small problem instances and good results in the medium instances. In the large problem instance, the static memory RL produced a better result than its counterpart. When comparing to other Great Deluge-based works, namely EGD, NLGD, ENLGD, and GD, the static memory RL produced the best solution in all problem instances except the large instance, in which its solution's score has a difference of only 1 from the best solution, which was generated by the Extended Great Deluge. The dynamic memory RL produced the worst solution among the algorithms compared for the large problem, but came in second overall in all of the medium problem instances, but one in which it came in third. When comparing the algorithm to hyper-heuristic-based algorithms with the same problem instances, the algorithm, specifically the static memory RL-based algorithm, produced the best results, except for the large instance. The authors also noted that the value of $lp$ affects the quality of the best solution the algorithm produces, with the sweet spot being either $lp = 2500$ or $lp = 5000$. When the algorithm is finally compared to course timetabling algorithms that are known to produce the best results for a specific problem instance, the algorithm, specifically the static memory RL one, produced new best solutions in all medium problem instances. This proves that the algorithm is a viable solution for course timetabling.

\section{Genetic Algorithm-Based Works}
Great Deluge is a metaheuristic \cite{intro-gd-dueck} that has been used for solving course timetabling problems \cite{gd-burke}\cite{nlgd-landa-silva}\cite{nlgdrl-obit}. Aside from Great Deluge, another metaheuristic that has seen use in the course timetabling problem and its variations is the Genetic Algorithm ((add other citations on GA)). The genetic algorithm is an optimization algorithm whose behaviour is based on how nature works, particularly on how reproduction works at a genetic level. In the algorithm, an initial population is first generated. This population does not necessarily contain the locally optimal solution but it is where the relatively best solution will be obtained from. This initial population is referred to as the first generation. From this population, a certain number of individuals will be randomly selected to be bred with one another and be the parents of the next generation. Selection of individuals is dependent on an individual's fitness, with the most fit individuals usually being selected. Calculating this fitness is dependent on the problem. In the context of course timetabling, fitness is based on the constraints that have been violated by the current solution/timetable generated \cite{alves-novel-recursive}\cite{raghavjee-ga-south-africa}\cite{johan-ga-sa-comparison}\cite{yik-ga-timetabling}\cite{supachate-noval-approach-ga-thai}\cite{wutthipong-performance-study-genetic-operators}\cite{sanjay-an-application-of-ga}. The breeding process involves selection of parents and having genes from the parents crossover and/or mutate to produce new offspring. Crossover is when genes from both parents are combined to produce an offspring. On the other hand, mutation is done by changing a random gene from either parent to create an offspring. Determining which genes from either parent to apply onto the offspring and which to mutate is dependent on implementation. Once a population of new generations is established, the cycle repeats. This continuous reproduction of generations eventually produces solutions "moves" towards the optimal solution \cite{what-is-ga}. Despite being able to produce feasible solutions for university timetabling, it should be noted that using a genetic algorithm approach may require more time executing compared to other approaches due to its population-based property. When compared to simulated annealing, another approach for university timetabling, the genetic algorithm takes more time executing \cite{johan-ga-sa-comparison}. However, approaches utilizing the genetic algorithm can see an improvement in execution time when they utilize graphics processing units (GPUs). The work of Yousef, A., Salama, C., Jad, M., El-Gafy, T., Matar, M., and Habashi, S. showed that it is possible to speed up genetic algorithms using GPUs. In their work, they accelerated the computation of the fitness function. Their experiments show that execution speed can be improved by up to 59 times in very large problem instances and by 280\% overall when utilizing the GPU \cite{yousef-gpu-ga}.

A work that utilizes genetic algorithms is the work of Alves, S., Oliveira, S., and Rocha Neto, A. \cite{alves-novel-recursive}. Unlike the previous works which tackles the problems as organizing a single set of lectures onto a table of time slots (which can be thought of timetabling for a semester), Alves. S., et. al.'s work organizes a timetable for entire courses throughout multiple semesters. We refer to a course here as a degree a student takes up in college. It is important to note that the perspective in which they tackle the problem is that of the Brazilian university academia's. Unique to their approach is that they only consider a single course in timetabling at a time. This means that the classes of other courses are not \textbf{directly} taken into account. Instead, what is considered is the unavailability of agents, which are students and professors. This unavailability determined from the assignments of the agents from the timetabling of the previous courses. Each course will contain timetables for each semester. In constructing the timetable for a course, a genetic algorithm is applied on each course, with each individual being a timetable for the course. Specific rules for selection of parents have not been specified in the paper but it can be presumed that the fittest individuals are chosen for breeding. Fitness $F$ is computed using the following function:

\[
	F(C) = 1 - \frac{AM_{C} + AU_{C} + AL_{C}}{AM_{wc} + AU_{wc} + AL_{wc}}
\]

$C$ is a course timetable. $AM$ represents the number of times an agent has been assigned to concurrent events. $AU$ stands for the number of times agents have assigned to classes whose timeslots they are not available in. Lastly, $AL$ represents the number of times a class has been given more than three consecutive timeslots. The numerator represents the values for $C$, while the denominator denotes the worst case values of those variables. Crossover is performed using the OX operator from the work of Chinnasri, W., Krootjohn, S., and Sureerattanan, N. \cite{wutthipong-performance-study-genetic-operators}. Mutation is performed in a course by selecting a random semester timetable and swapping two random timeslots. The timetable of a course is then stored in a global timetable. This timetabling is performed until all courses have been timetabled. The final solution is the global timetable. According to the experimental results, the approach of Alves, S., et. al. produces timetables that have a fitness close to $1$. However, producing fit timetables required performing the approach multiple times with different values. In their work, the authors obtained the the parameter values by performing 15 tests with each test having different values for the parameters. It should be highlighted that the stop criteria has been set to when a fitness of 1 has been achieved or the execution time has reached 10 minutes. Alves, S., et. al. found that, in their problem context, the parameters that produced the best results are: (a) 25\% mutation rate, (b) 50\% OX crossover rate, and (c) a population size of 75. Using these parameters, the authors managed to have their approach produce 493 generations and having a run time of just 3.7 minutes, with all timetables for each course having a fitness of approximately $1$.

Innet, S. has also utilized a genetic algorithm for timetabling \cite{supachate-noval-approach-ga-thai}.

%
%
%
%
%
%
%
%
%