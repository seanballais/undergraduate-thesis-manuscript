\chapter{Review of Related Literature} \label{sec:back}
Image vectorization has been a long and well-researched field. One of the earliest works on the field dates back to 1982 with the paper by Jimenez, J. and Navalon, J. Their works have been focused on vectorizing digital images from natural scenes \cite{somexperimentsvectorization}. The techniques they have used, such as contour following, have been adapted in later works. Later works have made significant progress over their work and made use of hardware advancements such as GPU utilization for computational tasks.

The primary goal of vectorization is to support multiple resolutions without any loss in the original data. Vectorization is suitable for the task. However, various methods have been proposed that do not involve vectorization. In this chapter, we will explore the previous works done for vectorization and making images support larger resolutions.

\section{Image Upscaling} \label{sec:image-upscaling}
Raster images are commonly used in representing images. Therefore, these types of images tend to be scaled to different resolutions. The process of scaling these types of images to higher resolutions is called \textit{image upscaling} \cite{hoshyari2018perceptiondriven}, and is the most common forms of having raster images support larger resolutions. Naively scaling images would result in images with empty pixels between the shifted pixels, producing a mostly empty image. Thus, the empty pixels are coloured based on their position from, and colour and intensity values of their neighbouring pixels. This process is called \textit{interpolation} \cite{resizingimages}. Traditional interpolation algorithms can be classified into two types: Non-adaptive techniques, and adaptive techniques. Each of these types have their own method of resizing images and would produce different results, which may fit in with different objectives of different users \cite{interpolationtechniquessurvey}. Most recent works uses artificial intelligence to construct a scaled image with as little image quality loss as possible. These works uses neural networks, specifically convolutional and adversarial neural networks, to produce photorealistic upscaled versions of images with little to no distinguishable loss of image quality \cite{aigigapixelstory}\cite{progressivesisr}\cite{sisrgan}.

\subsection{Non-Adaptive Techniques}
Non-adaptive interpolation techniques only scale images horizontally, vertically, or both, by a certain scaling factor, the amount in which the image will be scaled to. They do not assume anything about the underlying image data, except that it is band-limited \cite{depixelizingpixelart}. This makes them computationally cheap \cite{interpolationtechniquessurvey}, but also suffer from artifacts such as sharp edge blurring and ringing artifacts \cite{depixelizingpixelart}. Nevertheless, these non-adaptive image interpolation techniques have been widely used in the industry and is standard across different raster image editing programs, such as Adobe Photoshop and GIMP \cite{photoshopinterpolationmethods}\cite{gimpinterpolationmethods}. The common non-adaptive interpolation techniques used are:

\begin{itemize}
	\item Nearest Neighbour
	\item (Bi)linear Interpolation
	\item (Bi)cubic Interpolation
\end{itemize}

\subsubsection{Nearest Neighbour}
Nearest Neighbour is the simplest interpolation method to understand. At the high-level, nearest neighbour simply enlarges the size of each individual pixel by a certain factor.

At the lowest level, though this \textbf{may} differ implementation-wise, the algorithm will refer back to the original unscaled version to obtain the appropriate colours for each empty pixels in between the shifted pixels in the scaled image. Each empty pixel in the scaled version will map itself to a corresponding pixel in the original image. This is done by obtaining the square coordinates of the empty pixel and dividing the coordinates by the scaling factor. Note that the coordinate system in images starts its origin from the top left, instead of the bottom left. We may acquire a decimal as a result of dividing the coordinates, of which we will floor the values. The resulting coordinates can be mapped to a pixel in the original image. We will then use the mapped pixel's colour as the colour of the empty pixel we are calculating the colour for. The interpolated pixels can be seen as a set $I$, as quantified by the equation below.

$$ I = \{i | i = C((\lfloor\frac{e_{x}}{s}\rfloor, \lfloor\frac{e_{y}}{s}\rfloor))\}, s > 1, e \in E\ $$

$s$ is the scaling factor, $C$ is the function that gets the colour of a pixel based on its coordinate, and $E$ is the set of empty pixels that are in between the shifted pixels when naively upscaled. The resulting image will look blocky since the pixels are just enlarged. However, this will be ideal for pixel art \cite{resizingimages}.

\subsubsection{(Bi)linear and (Bi)cubic interpolation}\label{sec:bilinear-bicubic-interpolation}
Both of (bi)linear and (bi)cubic interpolations schemes are fairly similar to one another. The colours between the empty pixels between them are based on the values of the colours of the shifted pixels.

Each empty pixel acquire most of their colour from the nearest shifted pixel and least from the farthest shifted pixel.

% Explain later

\subsection{Adaptive Techniques}
% Explain later

\subsection{Super Resolution Methods}
Super resolution methods is the process of generating high-resolution images from low-resolution image input. This allows 
% Explain later 

\section{Vectorization of Images}
Various methods have been proposed throughout the years in the pursuit of supporting larger resolution without any reduction in image quality through vectorization. Typically, vectorization methods target specific inputs, such as natural images or artist drawn images, as certain methods are unsuitable for different inputs. Hoshyari, S., et. al. states that vectorizations targeted at natural images frequently produce inconsistent results when applied to artist-drawn imagery such as logos, and simple graphic illustrations \cite{hoshyari2018perceptiondriven}. No matter what the methods used are, they produce vector graphics that will vary in quality, photorealism, and artistic look (view \cite{hierarchicaldiffusioncurves}, \cite{barendrecht2018locally}, and \cite{anovelmethodforvectorization} for a comparison of the results of various vectorization methods).

Despite vector graphics providing a compact and alternative form of representing \cite{realtimevectorizationgpu}, it is important to remember that due to the inherent characteristics of vector primitives where certain fine details of raster images cannot be accurately represented in vectorized form, vector images will only be giving approximations of the details of images \cite{optimizedgradientmeshes}. Nevertheless, finding the perfect balance for image level of detail and image vectorization is an endeavour that is left as an exercise for users \cite{anovelmethodforvectorization}.

% Talk about the fact that there are multiple possible vector outputs for a single raster image?
(Use for talking about Hoshyari's work) There are multiple possible vectorization outputs for a single raster image, with many outputs being similar to one another and are good enough for use. For this reason, technically, there is no "correct" solution and we can use outputs that we find good enough. However, most people already have an expectation of what the ???

Image vectorization can be dated as far back as the early 1990s, though experiments have started since 1982 at the earliest \cite{somexperimentsvectorization}. Commercial packages, both proprietary and open source, have image vectorization tools whose quality vary. The packages include Adobe Illustrator (Live Trace), Corel CorelDRAW (PowerTRACE), and Inkscape (based on Potrace \cite{inkscapepotrace}) \cite{hoshyari2018perceptiondriven}. Their wide adoption, though impressive, do not always immediately translate to quality vectorizations. The current available methods still have their own shortfalls and are still in active development. As a result, manual vectorization are still being performed in many industries that heavily require vectorizations \cite{vectorizationoflinedrawingspolyvector}.

\subsection{Natural Images Vectorization}
Many raster images widely used today are photographs. They contain fine details and lush colour depth that are prominent in the real world. These photographs are also called natural images in digital image processing. Increasing their resolutions would entail using image upscaling techniques (see \ref{sec:image-upscaling}), especially the standard classical approaches. This would give the chance of producing low quality images \cite{depixelizingpixelart} when super resolution methods are not utilized. Vectorization of these natural images are then an alternative to producing high quality higher resolution images.

Vectorization methods targeted at natural images consist the large body of work that deals with automatic natural imagery vectorization. The core method of these algorithms involves the reliance on edge detection and/or region segmentations to cluster large quantities of pixels together into larger regions \cite{depixelizingpixelart}. These regions are then filled with either a solid color \cite{anovelmethodforvectorization}, or gradients via gradient meshes \cite{optimizedgradientmeshes}\cite{barendrecht2018locally} or diffusion curves\cite{hierarchicaldiffusioncurves}.

% Talk about gradient meshes and diffusion curves.

\subsection{Vectorization of Semi-Structured Images and Artworks}
Artwork, especially its subset, semi-structured images, is undeniably widely used around the globe. They are used to convey information, and express ideas. Both purposes would infer the necessity to maintain the high, or at least good enough quality images of those artworks to properly fulfill their tasks. Vectorization of these images would ensure that the quality of the image is kept at any resolution possible without any degradation. In the pursuit for quality vectorizations, many papers have been proposed that target specific inputs (such as pixel art, or small resolution images) and give out varying vectorization results.

A framework that optimizes bezigons, closed paths composed of Bezier curves, to match their raster counterparts as much as possible was proposed by Yang, M., et. al. as a vectorization process. Their work takes bezigons as input, of which they obtain either using pre-existing vectorization methods or extracting them from the raster image by segmenting the image into a set of regions then fitting piecewise Bezier curves on the region boundaries. The input is called the \textit{initial bezigons}. These initial bezigons are then optimized to reflect the raster image as close as possible. In optimizing the bezigons, they use a non-linear optimization algorithm (to be referred as *NLOAs* from hereafter) such as NEWUOA and conjugate gradient. Being an optimization-centered process, their process requires a method to evaluate their optimizations. Evaluating the vectorization output would involve getting the rasterization of the output and comparing it with the original raster image. Since there are various rasterization functions, a good enough rasterization function that works well with NLOAs is required. Using discontinuous and piecewise rasterization functions yields poor results during optimization. As such, a continuous rasterization function is required. Yang, M., et. al. chose to use a rasterization approach that utilizes a hierarchical Haar wavelet representation. The key component, which we view as a primary contribution of their work, is the energy $E$ used to evaluate the vectorized outputs. $E$ is the sum two parts: (a) the data energy, and (b) the prior energy. The data energy refers to the distance of the input raster image and the rasterization of the vectorization output. The prior energy refers to the severity of unreasonable bezigons. These bezigons would typically fall under one of the following categories, as per the work's authors intensive experimentation: (a) self-intersection, (b) false corners with small angle variations, (c) short handles, and (d) twisted sections. A larger $E$ would indicate a more inaccurate vectorized output. As such, optimization of bezigons, as it is their work, would be to minimize $E$. Basing from their experiments, they produce high quality vectorizations that are superior to the results of Vector Magic and Adobe LiveTrace. However, the frameworks yields poor results when vectorizing noisy or low-resolution inputs. Assuming that the experimental results be any sign for the actual theoretical speed, the execution time of this ranges from 10 seconds to 10 minutes, depending on the complexity of the shapes being vectorized. It is important to note that their implementation is not optimal and is written in Python, which provides a significant overhead. An implementation in a static, compiled language is expected to yield faster optimization speeds \cite{effectiveclipartimagevectorization}. Basing from personal experience in hand vectorization, this framework resembles manual vectorization in that a bezigon is initially created that does not immediately match the raster image. The bezigon is later adjusted to fit the raster image boundaries. Being that they have proposed a framework, their work can be viewed as an optional and/or complimentary post-processing step to tweak pre-vectorized accuracy-ambiguous images, rather than a complete alternative or replacement of pre-existing and well-established vectorization methods, such as Adobe LiveTrace, and Potrace. At least one work, specifically that of Hoshyari, S., et. al., has already stated that their work is complimentary to this \cite{hoshyari2018perceptiondriven}. The only disadvantage to this framework is that it requires multiple iterations before settling on an optimal vector solution, which, consequentially, require some time to complete. It is also unclear whether the framework becomes more computationally expensive as the raster image size becomes larger as their paper does not indicate whether it is so or not.

Another form of digital art that is a target of vectorization is pixel art. Pixel art is a form of art where the level of detail is limited to the pixel level. This type of art tends to be blocky, and is reminisce of the art style of retro-era games. 

% Show reason why vector lines must be continuos and differentiable.